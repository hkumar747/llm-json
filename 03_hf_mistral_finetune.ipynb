{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter efficient fine-tuning Mistral-7b\n",
    "\n",
    "### Using Quantized LoRA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: We need a customized model for extracting structured data from permit notices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "# https://blog.paperspace.com/mistral-7b-fine-tuning/\n",
    "# !pip install transformers trl langchain accelerate torch bitsandbytes peft datasets -qU\n",
    "# !pip install -q datasets loralib sentencepiece xformers einops\n",
    "!pip install -q -U peft==0.6.2 transformers==4.35.2 datasets==2.15.0 bitsandbytes==0.41.2.post2 trl==0.7.4 accelerate==0.24.1 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Efficient Fine Tuning with LoRA\n",
    "\n",
    "\n",
    "https://heidloff.net/article/efficient-fine-tuning-lora/\n",
    "\n",
    "https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1\n",
    "\n",
    "`verbatim`\n",
    "PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models to downstream applications without without the need to re-train or fine-tune all the parameters. \n",
    "\n",
    "One main form of PEFT is Low Rank Adaptation (LoRA) of large language models, based on the concept of rank of matrices in linear algebra.\n",
    "\n",
    "```\n",
    "LoRA is a method for fine-tuning language models without altering the original model parameters. In practical fine-tuning tasks, a set of low-rank adapters is added alongside specific model layers to be trained. In the original paper, the adapters were only added to two attention layers. The output dimensions of these adapters match those of the original model layers exactly.\n",
    "\n",
    "Subsequently, the adapters are set to be trainable, while the original model parameters are frozen and not allowed to be trained. This approach allows for the training of large language models without affecting inference speed significantly and only slightly increasing the parameter count.\n",
    "```\n",
    "\n",
    "\n",
    "\"common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space.\"\n",
    "https://arxiv.org/abs/2012.13255\n",
    "\n",
    "\n",
    "Instead of adjusting the entire weight matrix (which is part of the linear transformations in the model's layers), LoRA introduces additional low-rank matrices that modify the original weight matrices in a more parameter-efficient manner. This process allows for targeted adaptations that significantly alter the model's behavior \n",
    "\n",
    "\n",
    "<img src=\"images/peft_heidloff.png\" alt=\"Descriptive text about the image\" width=\"600\"/>\n",
    "\n",
    "Source: [Niklas Heidloff](https://heidloff.net/article/efficient-fine-tuning-lora/)\n",
    "\n",
    "\n",
    "**SVD and matrix rank**\n",
    "https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html\n",
    "\"The overall idea and concept are related to principal component analysis (PCA) and singular value decomposition (SVD), where we approximate a high-dimensional matrix or dataset using a lower-dimensional representation. In other words, we try to find a (linear) combination of a small number of dimensions in the original feature space (or matrix) that can capture most of the information in the dataset.\"\n",
    "\n",
    "**Remember:** Large Language Models (LLMs) are simply lots of matrices (or tensors) of numbers.\n",
    "\n",
    "\n",
    "\n",
    "**PEFT supported models**\n",
    "\n",
    "HuggingFace has a list of 10 models for which LoRA adapters are available:\n",
    "https://huggingface.co/docs/peft/index#supported-models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a model\n",
    "\n",
    "\n",
    "Look at the [HF leaderboard of open LLMS](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "\n",
    "There are multiple benchmarks to score LLMs, each designed for different linguistic, reasoning or numeric tasks.\n",
    "\n",
    "\n",
    "**Benchmarks:**\n",
    "\n",
    "\n",
    "**1. ARC (AI2 Reasoning Challenge)**\n",
    "ARC challenges models on grade-school level multiple-choice science questions. It's designed to test a model's reasoning and knowledge.\n",
    "\n",
    "\n",
    "**2. HellaSwag**\n",
    "HellaSwag is a benchmark for commonsense reasoning, predicting the endings of a given scenario in both textual and video contexts.\n",
    "\n",
    "\n",
    "**3. MMLU (Massive Multitask Language Understanding)**\n",
    "MMLU evaluates language models across a wide range of subjects and disciplines, testing the model's understanding on diverse topics.\n",
    "\n",
    "**4. TruthfulQA**\n",
    "This benchmark tests models on providing truthful answers, focusing on avoiding hallucinations and sticking to factual responses.\n",
    "\n",
    "\n",
    "**5. Winogrande**\n",
    "Winogrande is a dataset for commonsense reasoning, designed as an improved and scaled-up version of the Winograd Schema Challenge.\n",
    "\n",
    "\n",
    "**6. GSM8K (Grade School Math 8K)**\n",
    "GSM8K tests models on solving grade-school level math problems presented in textual form.\n",
    "\n",
    "\n",
    "Without admittedly delving too deep into it (maybe one of the above benchmarks has JSON tasks), my instinct would say TruthfulQA is more relevant for our task. It mostly doesn't require any external knowledge of the world (caveat: impacts on wetland), just the ability to not screw up when it transcribes from the text to the dictionary keys.\n",
    "\n",
    "In any case, we will be fine-tuning the Mistral-7b model.\n",
    "\n",
    "The following resources are helpful for understanding this notebook:\n",
    "\n",
    "- [Mistral](https://www.datacamp.com/tutorial/mistral-7b-tutorial)\n",
    "\n",
    "- [Mixtral](https://github.com/brevdev/notebooks/blob/main/mixtral-finetune-own-data.ipynb)\n",
    "\n",
    "- [huggingFace Transformers Training](https://huggingface.co/docs/transformers/en/training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights & Biases for tracking metrics\n",
    "\n",
    "Use Weights & Biases to track training metrics. Enter your API key when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"mistral-7b-usace-finetune_v1\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training dataset\n",
    "\n",
    "It just needs to be a dataset of input-output pairs. The phrase 'supervised fine tuning' signals an image of a dataset of (vectors) of inputs and output, as in supervised learning. But that would be misleading - here the Xs and Ys are concatenated together into strings of continuous tokens, interspersed with special tokens which marks inputs and outputs.\n",
    "\n",
    "Basically, my dataset consists of input-output pairs like:\n",
    "\n",
    "**Input**\n",
    ">The applicant seeks author ization to construct a single-family residence on the 2.09-acre parcel, including the permanent fill and loss of  0.78 acres of mangrove wetlands.\n",
    "\n",
    "\n",
    "**Output**\n",
    ">\n",
    "\n",
    "{'wetlands': [\n",
    "  {'wetland_type': 'mangrove wetlands',\n",
    "  \n",
    "   'impact_quantity': 0.78,\n",
    "\n",
    "   'impact_unit': 'acres',\n",
    "\n",
    "   'impact_type': 'fill',\n",
    "   \n",
    "   'impact_duration': 'permanent'}]}\n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "instruct_tune_dataset\n",
    "\n",
    "type(instruct_tune_dataset)\n",
    "\n",
    "instruct_tune_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convert from OpenAI JSONL to HuggingFace Dataset\n",
    "\n",
    "https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
    "\n",
    "\n",
    "Load the JSONL file of verified input-output pairs we used in notebook 2. (This step assumes you've already connverted them into JSONL format for OpenAI.) The following step will convert these into a format suitable for HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "def load_jsonl_to_dataset(jsonl_file_path):\n",
    "    # Initialize lists to hold the values for each field\n",
    "    system_messages = []\n",
    "    user_messages = []\n",
    "    assistant_messages = []\n",
    "\n",
    "    # Open and read the JSONL file\n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            system_msg = user_msg = assistant_msg = None  # Reset for each entry\n",
    "\n",
    "            # Iterate over messages and extract content based on role\n",
    "            for message in data['messages']:\n",
    "                if message['role'] == 'system':\n",
    "                    system_msg = message['content']\n",
    "                elif message['role'] == 'user':\n",
    "                    user_msg = message['content']\n",
    "                elif message['role'] == 'assistant':\n",
    "                    assistant_msg = message['content']\n",
    "\n",
    "            # Append messages to their respective lists\n",
    "            system_messages.append(system_msg)\n",
    "            user_messages.append(user_msg)\n",
    "            assistant_messages.append(assistant_msg)\n",
    "\n",
    "    # Construct a dictionary with these lists\n",
    "    data_dict = {\n",
    "        'system': system_messages,\n",
    "        'user': user_messages,\n",
    "        'assistant': assistant_messages\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = load_jsonl_to_dataset(\"usace_finetune_training.jsonl\")\n",
    "val_dataset = load_jsonl_to_dataset(\"usace_finetune_validation.jsonl\")\n",
    "\n",
    "# Displaying the structure of the train dataset as an example\n",
    "print(f\"Train Dataset:\\n{train_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formatting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    bos_token = \"<s>\"\n",
    "    eos_token = \"</s>\"\n",
    "\n",
    "    # Directly access the 'system', 'user', and 'assistant' messages from the sample\n",
    "    system_message = sample['system'].replace(\"\\n\", \" \").strip()\n",
    "    user_message = sample['user'].replace(\"\\n\", \" \").strip()\n",
    "    assistant_message = sample['assistant'].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # Concatenate the prompt according to the specified format\n",
    "    full_prompt = f\"{bos_token}\"\n",
    "    full_prompt += f\"### Instruction: \\n{system_message}\"\n",
    "    full_prompt += f\"\\n\\n### Input:\\n{user_message}\"\n",
    "    full_prompt += f\"\\n\\n### Response:\\n{assistant_message}\"\n",
    "    full_prompt += f\"{eos_token}\"\n",
    "\n",
    "    return full_prompt\n",
    "create_prompt(train_dataset[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Train the Model\n",
    "\n",
    "https://blog.paperspace.com/mistral-7b-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Quantization (QLoRA)\n",
    "\n",
    "QLoRA by Dettmers et al., short for quantized LoRA, is a technique that reduces memory usage during finetuning. During backpropagation, QLoRA quantizes the pretrained weights to **4-bit precision** and uses **paged optimizers** to handle memory spikes.\n",
    "\n",
    "This author found he saved 33% of GPU memory when using QLoRA, at a cost of 39% increased training runtime. This is caused by the additional quantization/dequantization steps of the pretrained model weights in QLoRA.\n",
    "\n",
    "**Note:** Its preferred to use BFloat16, but it wasn't supported on a V100 GPU on Colab, so I worked around this.\n",
    "\n",
    "https://brev.dev/blog/how-qlora-works\n",
    "\n",
    "The above article raises a very good point about the democratization of AI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "  #  bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# \"stabilityai/stablelm-3b-4e1t\" - No quantization?\n",
    "\n",
    "# 'EleutherAI/gpt-neo-1.3B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "# ,\n",
    "    device_map='auto',\n",
    "    load_in_4bit=True,\n",
    "\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #    padding_side=\"left\",\n",
    "    # add_eos_token=True,\n",
    "    # add_bos_token=True,\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization\n",
    "\n",
    "Before training, you need to tokenize your dataset. The tokenizer will convert your text into a format that's suitable for the model to process:\n",
    "\n",
    "\n",
    "**Newline Characters**\n",
    "\n",
    "- When tokenizing text, newline characters are treated as whitespace and are used to separate tokens. For many models, especially those trained on a wide variety of internet text (like GPT, BERT, etc.), encountering newline characters is expected and won't cause issues.\n",
    "\n",
    "- For some tasks, newline characters might carry semantic meaning (e.g., separating paragraphs or items in a list), which could be relevant for the model to understand the structure of the input text.\n",
    "\n",
    "\n",
    "**Note:** Padding - affects compute requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = \"\"\n",
    "\n",
    "# set max length of sequence\n",
    "max_length=968\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     max_length=max_length,padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a histogram of the distribution of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     max_length=max_length,padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set LoRA Configuration\n",
    "\n",
    "\n",
    "Choose the particular layers you want to fine tune - the more layers, the more the compute cost and training time, maybe for negligibly higher performance.\n",
    "\n",
    "As mentioned above, the original paper chose the Q and V layers of the attention mechanism.\n",
    "\n",
    "r = rank of the low-rank matrix used in the adapters. Changes the number of parameters trained. In the extreme, full rank would mean full fine-tuning \n",
    "\n",
    "alpha = scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were r=64 and lora_alpha=16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.08,\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\"],\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    #     \"w1\",\n",
    "    #     \"w2\",\n",
    "    #     \"w3\",\n",
    "    #     \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare model for kbit training\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN this step to see the model - introduces errors\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many trainable parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters\n",
    "\n",
    "\n",
    "Use the `transformers` trainer. This is not the full list of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"mistral_instruct_generation\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 100,\n",
    "  per_device_train_batch_size = 1,\n",
    "  gradient_accumulation_steps = 4,\n",
    "  # gradient_checkpointing = True,\n",
    "  fp16 = True,\n",
    "  # gradient_checkpointing_steps = 10,\n",
    "  warmup_steps = 0.03,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=40,\n",
    "  learning_rate=2e-4,\n",
    "  bf16=False,\n",
    "  lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "\n",
    "Use the huggingface `trl` module. Remember to replace create_prompt with the specific function for formatting prompts that you used above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  dataset_text_field=\"text\",\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset= val_dataset,\n",
    "  # instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.model.save_pretrained(\"mistral_ft_8mar\")\n",
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "from transformers import pipeline\n",
    "prompt = \"Extract to JSON: The applicant has requested Department of the Army authorization to clear, grade and fill in order to expand an ex isting sand and gravel mining operation in Grangeville, Louisian a.The proposed sand and gra vel pits will encompass approximately 56.5 ac res and will be dug to adepth of 35 feet with 3:1 side slopes.Approximately 47,430 cubic yards of dirt, sand and gravel will be excavated and placed on upland areas of the site.It is anticipated that the proposed activity will impact approximately 1.47 acres of forested wetlands.It is presumed that the applicant has designed the project to a void and minimize direct and secondary adverse impacts tothe maximum extent practicable .As compensation forunavoidable wetland impacts, the applicant proposes to mitigatein-kind wetland credits from a Corps approved mitigat ion bank located in the watershed.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=600)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
